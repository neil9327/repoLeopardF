{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of Git. Commit 10\n",
    "\n",
    "# Instructions\n",
    "# Download the Jupyter notebook containing the code: LeopardA.ipynb\n",
    "# and open it (on Colorado) in VS Code.\n",
    "# Create a python environment (in Anaconda), or use the existing environment on Anaconda on Colorado named: envLeopard\n",
    "# Install the four packages listed at the start of the file into that environment:\n",
    "# \t# pip install yfinance\n",
    "# \t# pip install tensorflow\n",
    "# \t# pip install scikit-learn\n",
    "# \t# pip install matplotlib\n",
    "#   # pip install google.auth\n",
    "#   # pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
    "\n",
    "# Copy the model file: modelLeopardA.keras\n",
    "# and the source data file: LeopardA.csv\n",
    "# into a local directory, if necessary updating the corresponding directories in the code to point to them.\n",
    "# Scroll down to the bottom of the code, and update variable iRunMain = 0\n",
    "# Run the notebook by clicking \"Run All\" at the top of VS Code.\n",
    "# Next, change iRunMain to equal 1, \n",
    "# Next run the second from last field by clicking debug cell, then place a red breakpoint marker just above: print(oPredicted)\n",
    "# Then run debug cell on the last field, to start the program running properly.\n",
    "\n",
    "# It will take a minute or so to complete, running the existing model to generate predictions on the 200 Test records.\n",
    "\n",
    "# Once it hits the breakpoint, we can take a look at the running variables.\n",
    "\n",
    "# The question is: Can the code detect the pattern that I have put in the source data?\n",
    "\n",
    "# First look at array: nbxTest4D\n",
    "# Within this, go to record [14], and look at the 20 items within this. The lowest item ([19]) = -0.00051344\n",
    "# and the other items at lower numbers (corresponding to data further to the left) are all less than 0.001\n",
    "\n",
    "# Next to to record [15], and look at the 20 items. Here item [19] = -0.019608\n",
    "# which is a decline of 2% in the way I've done it.\n",
    "# This is the pattern that should be detected, which is that every time you get a decline of 2%, there should be a corresponding increasse of 10% seven time periods later.\n",
    "\n",
    "# So is this predicted?\n",
    "\n",
    "# To see whether this is the case, go to array: nPredictedFinal\n",
    "# and go to record [14]. Look at the 20 records within it, and note that the values are all small (<0.01).\n",
    "# Now go to record [15], and look at the 20 records there. There we can see that up to record [5] the values are small. But from [6] onwards it is 0.0984 - i.e. around 0.1 or 10%. So the prediction has worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LeopardF sample data documentation\n",
    "# **********************************\n",
    "# \n",
    "# So for price, it is 29-30\t(7 forwards)\n",
    "# For Social1, it is 38-39\t(10 forwards)\n",
    "# For Social2 it is 02-03\t\t(15 forwards)\n",
    "# \n",
    "# Source file: LeopardF_100000_sample_data.csv\n",
    "# contains sample data with patterns I have created manually, for 10 stocks each with 10,000 Runs. So 100,000 rows in total, plus the header row.\n",
    "# It contains the following columns:\n",
    "# \n",
    "# viShareId\t\t\tInteger ID from 10 to 19\n",
    "# viRunId\t\t\t\tInter runID from 1 to 10000\n",
    "# vsTicker\t\t\tS1 to S9\n",
    "# vdtDateTime\t\t\tdatetime\n",
    "# vfPrice\t\t\t\tprice as float. Note this is only used as a label in the script (no calculation is done on it).\n",
    "# vfPriceChange\t\tRatio of the previous price to the next, minus one. This has been calculated in the source data. So values are like 0.0025 or 0.1 or -0.004\n",
    "# vfSocial1Change\t\tSocial ratio 1\n",
    "# vfSocial2Change\t\tSocial ratio 2\n",
    "# \n",
    "# This is loaded into a pandas array, and the 6 numerical ones (all but vsTicker and vdtDateTime) are loaded into a numpy array: nAAll2D\n",
    "# \n",
    "# This is split into training and test sets with 9800 and 200 records each.\n",
    "# X and Y datasets are created from these, where each row in X is made up of not just the current record, but the previous 20 records, and each row in Y is the cumulative total of each of the following 20 records.\n",
    "# Each training row is therefore training on the effect on the share price at every point from 1 to 20 in the future.\n",
    "# So the X train dataset nBXtrain3D has shape (98800, 20, 3)\t(with the 3 being price change and social 1 and social 2), and the Y train dataset nBYtrain3D has shape (98800, 20, 1) - just the cumulative pricechange.\n",
    "# Note that in both X and Y datasets, the numbering goes from earliest to latest. So X[19] is closest to the X value (looking backwards), and Y[0] is closest to the Y value (looking forwards).\n",
    "# \n",
    "# \n",
    "# Once the model is trained on the training data, the predictions are run on the test data.\n",
    "# The test X dataset nBXAlltest4D has shape (10,160,20,6). Note this is 160 not 200 because we need to go 20 forward and 20 back from each data point to get all the data we need - so 40 rows have to be rejected as primary test records.\n",
    "# The 6 records are: stock, run, price, pricechange, social1, social2\n",
    "# \n",
    "# \n",
    "# When the predictions are run, these predictions are saved into a copy of the test X dataset nBXAlltest4D, named nPredictions4D. This has the predicted value saved in the 7th field, and the actual cumulative value (from Y) in the 8th field.\n",
    "# This is a slight conceptual bodge because it implies in error there is a connection between the pricechange and socials values in X, and the predicted values and actual values in Y.\n",
    "# \n",
    "# I am defining the four dimensions as being:\n",
    "# \tStock\t(0-9)\n",
    "# \tRun\t\t(0-199)\n",
    "# \tSpread\t(0-19)\n",
    "# \tAsset\t(0-7)\n",
    "# \n",
    "# nPredictions4D[StockIndex(0-9), RunIndex(0-199), twenty spread (0-19), asset]\n",
    "# where the above values are:\n",
    "# \n",
    "# \t0\tviShareId\t\t\tInteger ID from 10 to 19\n",
    "# \t1\tviRunId\t\t\t\tInter runID from 1 to 10000\n",
    "# \t2\tvfPrice\t\t\t\tprice as float. Note this is only used as a label in the script (no calculation is done on it).\n",
    "# \t3\tvfPriceChange\t\tRatio of the previous price to the next, minus one. An X value. This has been calculated in the source data. So values are like 0.0025 or 0.1 or -0.004\n",
    "# \t4\tvfSocial1Change\t\tSocial ratio 1. An X value.\n",
    "# \t5\tvfSocial2Change\t\tSocial ratio 2. An X value.\n",
    "# \t6\tPrediction\n",
    "# \t7\tActual Y\n",
    "# \n",
    "# Note that StockIndex and RunIndex are not the same as StockId and RunId. This is because you can have gaps in the StockId and RunId (due to missing data etc) but you cannot have gaps in StockIndex and RunIndex - which must start at 0 and rise without gaps.\n",
    "# \n",
    "# This is all best explained with examples.\n",
    "# \n",
    "# \n",
    "# Price spike\n",
    "# ***********\n",
    "# Open the above csv file, and scroll down to line 29852.\n",
    "# This has data for the third stock (stockID=12 referenced as 2), for run 29852. And we can see here that pricechange is 0.02 - which is much higher than the typical random values of circa 0.001 etc.\n",
    "# Consistent with the pattern in this data, 7 lines later at run 29859 the pricechange value spikes to 0.1\n",
    "# We want therefore to see that the model has predicted this rise, and how this prediction is stored in nPredictions4D along with the actual Y value.\n",
    "# \n",
    "# Looking at nPredictions4D (which has shape (10,200,20,6)), we want to look at the first record where the 0.02 change first appears as an X value\n",
    "# So take line 29852, subtract 1 for the header row to give 29851, then subtract 20000 to give 9851 to have just the rows for this stock\n",
    "# , and then subtract 9800 to remove the training data (leaving only the 200 for the test set) gives 51\n",
    "# And then subtract 20, to get to 31 to reflect the fact that the data has to start (and end) 20 items in to the data, in order to ensure there are always 20 runs prior for X, and following for Y.\n",
    "# So we want to find the 31st record in nPredictions4D. Which is referenced as (30) as we start from 0.\n",
    "# And as the 0.02 record appears for the first time, it will be present at the most recent point in the 20, which is referenced as 19.\n",
    "# \n",
    "# So let's run:\n",
    "#   fPrint(nPredictions4D[2, 30, 19, 3])\t#0.02\n",
    "# to verify that this is indeed the 0.02 value that triggers the detection of the price change. It is.\n",
    "# \n",
    "# (note that I use function fPrint, that I have defined earlier in the script, to do the printing, in order to stop the use of exponents E01 etc in the output). So: fPrint(nPredictions4D[2, 30, 6, :])\n",
    "# \n",
    "# Next move 7 runs forward, to verify that the latest pricechange is indeed 0.1:\n",
    "#   fPrint(nPredictions4D[2, 37, 19, 3])\t#0.1\n",
    "# \n",
    "# Next go back to nPredictions4D[2,30,19,3], and now look at the actual value (Y value) of the price for 6 and 7 spreads into the future (these are spreadindexes 5 and 6), and verify that the second is around 0.1 higher than the first.\n",
    "#   fPrint(nPredictions4D[2,30,5,7])\t#0.0004791183753658501\n",
    "#   fPrint(nPredictions4D[2,30,6,7])\t#0.10052703021290244\n",
    "# These output:\n",
    "# \t0.0004791183753658501\n",
    "# \t0.10052703021290244\n",
    "# So the difference between these (remember they are cumulative) is around 0.1\n",
    "# \n",
    "# Next, is this change predicted? To see this, run on index 6 (predicted values) rather than 7 (actual Y values):\n",
    "#   fPrint(nPredictions4D[2,30,5,6])\t#0.0011756770545616746\n",
    "#   fPrint(nPredictions4D[2,30,6,6])\t#0.09948894381523132\n",
    "# so yes it is.\n",
    "# \n",
    "# Finally let's go back one run to before the 0.02 rise, and verify that the rise is NOT predicted.\n",
    "# (note we have to increment the spread by one)\n",
    "#   fPrint(nPredictions4D[2,29,6,6])\t#0.0052693127654492855\n",
    "#   fPrint(nPredictions4D[2,29,7,6])\t#0.008663389831781387\n",
    "# \n",
    "# and that of course the Y value do still have the rise:\n",
    "#   fPrint(nPredictions4D[2,29,6,7])\t#0.0204887007428729\n",
    "#   fPrint(nPredictions4D[2,29,7,7])\t#0.12253757081716032\n",
    "# \n",
    "# \n",
    "# Social1 spike\n",
    "# *************\n",
    "# The same principle applies with the social spike 1 a little later than the price spike example)\n",
    "# So in the above CSV file go to line 29861\n",
    "# Here we can see that vfSocial1Change is shown as: -0.019608000000000\n",
    "# (which is higher than the typical values in this column).\n",
    "# Then go forward 10 lines to 29871 and see that there is a price spike of -0.090909090909091\n",
    "# \n",
    "# So to see the first social change input X values, let's run the same first query, but on run 39 instead of 30, and :\n",
    "# \tfPrint(nPredictions4D[2, 39, 19, 4])\t#-0.019608\n",
    "# Then 10 lines further down, look at the price change:\n",
    "# \tfPrint(nPredictions4D[2, 49, 19, 3])  #-0.090909090909091\n",
    "# \n",
    "# Now go back to fPrint(nPredictions4D[2, 39, 19, 4]), and now look at the actual value (Y value) of the price for 9 and 10 spreads into the future (these are spreadindex values 8 and 9), and verify that the second is around 0.1 lower than the first.\n",
    "#   fPrint(nPredictions4D[2, 39, 8, 7])   #0.021082481702138667\n",
    "#   fPrint(nPredictions4D[2, 39, 9, 7])   #-0.07174319845260135\n",
    "# So yes the change is here, with the difference being around -0.1 as they are cumulative.\n",
    "# \n",
    "# Next, is this change predicted? To see this, run the above on index 6 (predicted values) rather than 7 (actual Y values):\n",
    "#   fPrint(nPredictions4D[2, 39, 8, 6])   #0.010913881473243237\n",
    "#   fPrint(nPredictions4D[2, 39, 9, 6])   #-0.07884889841079712\n",
    "# so yes it is (remember we are interested in the difference here, as the values are cumulative).\n",
    "# \n",
    "# Finally let's go back one run to before the social change, and verify that the rise is NOT predicted.\n",
    "# (note we have to increment the spread by one)\n",
    "#   fPrint(nPredictions4D[2, 38, 9, 6])   #0.014400970190763474\n",
    "#   fPrint(nPredictions4D[2, 38, 10, 6])  #0.016602344810962677\n",
    "# and indeed it is not. Note that it is the -difference- between the above two we are interested in here.\n",
    "# \n",
    "# and that of course the Y value from that Run (38) does still have the 0.1 drop:\n",
    "#   fPrint(nPredictions4D[2, 38, 9, 7])   #0.02024992907694667\n",
    "#   fPrint(nPredictions4D[2, 38, 10, 7])  #-0.07250006447550317\n",
    "# \n",
    "# \n",
    "# Social2 spike\n",
    "# *************\n",
    "# Social spike 2 is seen in the CSV file on line 29825\n",
    "# where we can see that vfSocial1Change is shown as:\t-0.019608000000000\n",
    "# Then go forward 15 lines to line 29840, and see that there is a price spike of: -0.090909090909091\n",
    "# \n",
    "# So to see the first social change on line 29825, run the following to see the input X:\n",
    "#   fPrint(nPredictions4D[2, 3, 19, 5])\t#-0.019608\n",
    "#   fPrint(nPredictions4D[2, 18, 19, 3])\t#-0.090909090909091\n",
    "# \n",
    "# Now go back to fPrint(nPredictions4D[2, 3, 19, 5]), and now look at the actual value (Y value) of the price for 14 and 15 spreads into the future (these are spreadindex values 13 and 14), and verify that the second is around 0.1 lower than the first.:\n",
    "#   fPrint(nPredictions4D[2, 3, 13, 7])   #-0.0005855898365195733\n",
    "#   fPrint(nPredictions4D[2, 3, 14, 7])   #-0.09144144530592702\n",
    "# \n",
    "# Next, is this change predicted? To see this, run on index 6 (predicted values) rather than 7 (actual Y values):\n",
    "#   fPrint(nPredictions4D[2, 3, 13, 6])   #0.030499937012791634\n",
    "#   fPrint(nPredictions4D[2, 3, 14, 6])   #-0.060842156410217285\n",
    "# so yes it is (remember we are interested in the difference here, as the values are cumulative).\n",
    "# \n",
    "# Finally let's go back one run to before the social change, and verify that the rise is NOT predicted.\n",
    "# (note we have to increment the spread by one)\n",
    "#   fPrint(nPredictions4D[2, 2, 14, 6])   #0.018615230917930603\n",
    "#   fPrint(nPredictions4D[2, 2, 15, 6])   #0.018593236804008484\n",
    "# and indeed it is not. Note that it is the -difference- between the above two we are interested in here.\n",
    "# \n",
    "# and that of course the Y value from that Run (2) does still have the 0.1 drop:\n",
    "#   fPrint(nPredictions4D[2, 2, 14, 7])   #-0.0008223257331689826\n",
    "#   fPrint(nPredictions4D[2, 2, 15, 7])   #-0.09165665975742654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install yfinance\n",
    "# %pip install tensorflow\n",
    "# %pip install scikit-learn\n",
    "# %pip install matplotlib\n",
    "# %pip install pyodbc\n",
    "# %pip install google.auth\n",
    "# %pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pyodbc, struct\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_percentage_error # max_error #mean_absolute_percent_error\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.models\n",
    "import keras.layers\n",
    "import keras.layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "def fBuildModel(n_outputs, n_timesteps, n_features):\n",
    "    model = Sequential()\n",
    "    # input layer: this is taking our raw data and convoluting it, essentially its picking up salient features.\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps, n_features), padding='same')) \n",
    "    # this is our first hidden layer, its taking in the ouput of our input layer and finding subtle patterns in our features\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))  \n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    # time series magic occurs\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features), return_sequences=True))\n",
    "    model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features), return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu'))) # gonna change this from relu to softmax\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "def fPrint(nInputArray):\n",
    "    sInputArrayType = str(type(nInputArray))\n",
    "    # print(type(nInputArray))\n",
    "    if sInputArrayType == \"<class 'numpy.ndarray'>\":\n",
    "        sStringArrayToPrint = np.array_str(nInputArray, precision=8, suppress_small=True)\n",
    "    else:\n",
    "        sStringArrayToPrint = str(nInputArray)\n",
    "    print(sStringArrayToPrint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fGetSpread(nSpreadX3D, iDepthX, iDepthY) -> tuple[np.array, np.array, np.array, np.array]:\n",
    "  X, y, listX3D, listY3D, listX4D, listY4D = [], [], [], [], [], []\n",
    "\n",
    "  iNumStocksLocal = nSpreadX3D.shape[0]\n",
    "  iNumRecordsPerStockLocal = nSpreadX3D.shape[1]\n",
    "\n",
    "  # iLenX = nSpreadX3D.shape[0] #len(nSpreadX3D)\n",
    "\n",
    "  iStockIndex = 0\n",
    "  iRecord = 0\n",
    "  \n",
    "  for _ in range(iNumStocksLocal):\n",
    "    for _ in range(iNumRecordsPerStockLocal):\n",
    "      if iRecord >= iDepthX+1 and iRecord <= iNumRecordsPerStockLocal - iDepthY:\n",
    "        listRowX = nSpreadX3D[iStockIndex,iRecord-iDepthX:iRecord,:]\n",
    "        listX3D.append(listRowX)\n",
    "        listRowY = nSpreadX3D[iStockIndex,iRecord:iRecord+iDepthY,:]\n",
    "        listY3D.append(listRowY)\n",
    "      iRecord += 1\n",
    "    listX4D.append(listX3D)\n",
    "    listY4D.append(listY3D)\n",
    "    listX3D = []\n",
    "    listY3D = []\n",
    "    iRecord = 0\n",
    "    iStockIndex += 1\n",
    "\n",
    "  nXAll4D = np.array(listX4D) # contains share price, PriceChange, and social media\n",
    "  nXPriceChanges3D = nXAll4D[:, :, :, 3] # contains the PriceChange\n",
    "  nXPriceChanges4D = nXPriceChanges3D.reshape((nXPriceChanges3D.shape[0], nXPriceChanges3D.shape[1], nXPriceChanges3D.shape[2], 1))\n",
    "  nXSocialChanges3D = nXAll4D[:, :, :, 4:6] # contains the SocialChange\n",
    "  nXSocialChanges4D = nXSocialChanges3D.reshape((nXSocialChanges3D.shape[0], nXSocialChanges3D.shape[1], nXSocialChanges3D.shape[2], 2))\n",
    "\n",
    "  nX4D = np.concatenate((nXPriceChanges4D, nXSocialChanges4D), axis=-1)\n",
    "  nX3D = nX4D.reshape(nX4D.shape[0]*nX4D.shape[1], nX4D.shape[2], nX4D.shape[3])\n",
    "\n",
    "  nYAll4Draw = np.array(listY4D)\n",
    "\n",
    "  nYStocks3D = nYAll4Draw[:, :, :, 0]\n",
    "  nYRuns3D = nYAll4Draw[:, :, :, 1]\n",
    "  nYPrices3D = nYAll4Draw[:, :, :, 2]\n",
    "  nYPriceChanges3D = nYAll4Draw[:, :, :, 3]\n",
    "  nYPriceRatios3D = nYPriceChanges3D.copy()\n",
    "  nYPriceRatios3D[:, :] += 1\n",
    "\n",
    "  nYCumulativeByRatios3D = np.cumprod(nYPriceRatios3D, axis=2) \n",
    "\n",
    "  nYCumulative3D = nYCumulativeByRatios3D.copy()\n",
    "  nYCumulative3D[:, :] -= 1\n",
    "  \n",
    "  nYStocks4D = nYStocks3D[..., np.newaxis]\n",
    "  nYRuns4D = nYRuns3D[..., np.newaxis]\n",
    "  nYPrices4D = nYPrices3D[..., np.newaxis]\n",
    "  nYPriceChanges4D = nYPriceChanges3D[..., np.newaxis]\n",
    "  nYCumulative4D = nYCumulative3D[..., np.newaxis]\n",
    "\n",
    "  nYAll4D = np.concatenate((nYStocks4D, nYRuns4D, nYPrices4D, nYPriceChanges4D, nYCumulative4D), axis=-1)\n",
    "  nY4D = nYCumulative4D\n",
    "  nY3D = nY4D.reshape(nY4D.shape[0]*nY4D.shape[1], nY4D.shape[2], nY4D.shape[3])\n",
    "\n",
    "  return nXAll4D, nX4D, nX3D, nYAll4D, nY4D, nY3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fMain(iAction, sIsAzureYN, sModelFileName\n",
    "          , sSourceFileName, sPredictionsFileName, sTradesFileName, fPredictTarget\n",
    "          , iEpochs, iNumTestSplitRecordsPerStock, iDepthX, iDepthY):\n",
    "  # Step 1: Create the data\n",
    "  # iLenATest = 200\n",
    "  # iDepthX=20\n",
    "  # iDepthY=20\n",
    "  sSourceFileFullPath = \"\"\n",
    "\n",
    "  if sIsAzureYN == \"Y\":\n",
    "    sSourceFileFullPath = \"files/repoLeopard/sourcefiles/\" + sSourceFileName\n",
    "    sModelFileFullPath = \"files/repoLeopard/modelfiles/\" + sModelFileName\n",
    "    sPredictionsFileFullPath = \"files/repoLeopard/predictionsfiles/\" + sPredictionsFileName\n",
    "    sTradesFileFullPath = \"files/repoLeopard/tradesfiles/\" + sTradesFileName\n",
    "\n",
    "  if sIsAzureYN == \"N\":\n",
    "    sSourceFileFullPath = \"C:\\\\FILES\\\\IB_API_Dev\\\\Tiger\\\\repoLeopard\\\\sourcefiles\\\\\" + sSourceFileName\n",
    "    sModelFileFullPath = \"C:\\\\FILES\\\\IB_API_Dev\\\\Tiger\\\\repoLeopard\\\\modelfiles\\\\\" + sModelFileName\n",
    "    sPredictionsFileFullPath = \"C:\\\\FILES\\\\IB_API_Dev\\\\Tiger\\\\repoLeopard\\\\Predictionsfiles\\\\\" + sPredictionsFileName\n",
    "    sTradesFileFullPath = \"C:\\\\FILES\\\\IB_API_Dev\\\\Tiger\\\\repoLeopard\\\\Tradesfiles\\\\\" + sTradesFileName\n",
    "\n",
    "  dfSourceFile = pd.read_csv(sSourceFileFullPath)\n",
    "  dfSourceFileNumbersOnly = dfSourceFile.filter(['viShareId','viRunId','vfPrice','vfPriceChange','vfSocial1Change','vfSocial2Change'], axis=1)\n",
    "  \n",
    "  nAAll2D = dfSourceFileNumbersOnly.to_numpy()\n",
    "\n",
    "  # nAAll2Db = []\n",
    "  # for element in nAAll2Da:\n",
    "  #   print(element[2])\n",
    "  #   nAAll2Db.append(element)\n",
    "  # \n",
    "  # nAAll2D = np.array(nAAll2Db)\n",
    "\n",
    "  # nAAll2D = nAAll2Da[0:1,:]\n",
    "  iNumRecords = nAAll2D.shape[0]\n",
    "  oDiscard, oNumStocks = np.unique(nAAll2D[:, 0], return_counts=True)\n",
    "  iNumStocks = oNumStocks.shape[0]\n",
    "  iNumRecordsPerStock = iNumRecords // iNumStocks\n",
    "\n",
    "\n",
    "  nAAll3D = nAAll2D.reshape(iNumStocks, iNumRecordsPerStock, 6)  # was 5\n",
    "  nA3D = nAAll3D[:,:,2:6] # was 5\n",
    "  \n",
    "  iLenAAll = iNumRecordsPerStock # nAAll2D.shape[0] #len(nAAll) #= 100,000\n",
    "  iLenATrain = iLenAAll - iNumTestSplitRecordsPerStock # = 10,000 - 200 = 9,800\n",
    "\n",
    "  # nATrain3D = nA3D[:,0:iLenATrain,:]  \n",
    "  # nATest3D = nA3D[:,iLenATrain:,:]   \n",
    "\n",
    "  nATrain3D = nAAll3D[:,0:iLenATrain,:]  \n",
    "  nATest3D = nAAll3D[:,iLenATrain:,:]   \n",
    "\n",
    "  tupleTrainSpread = fGetSpread(nATrain3D, iDepthX, iDepthY)\n",
    "\n",
    "  nBXAlltrain4D    = tupleTrainSpread[0]\n",
    "  # nBXtrain4D       = tupleTrainSpread[1]\n",
    "  nBXtrain3D       = tupleTrainSpread[2]\n",
    "  nBYAlltrain4D    = tupleTrainSpread[3]\n",
    "  # nBYtrain4D       = tupleTrainSpread[4]\n",
    "  nBYtrain3D       = tupleTrainSpread[5]\n",
    "\n",
    "  tupleTestSpread = fGetSpread(nATest3D, iDepthX, iDepthY)\n",
    "\n",
    "  nBXAlltest4D     = tupleTestSpread[0]\n",
    "  # nBXtest4D        = tupleTestSpread[1]\n",
    "  nBXtest3D        = tupleTestSpread[2]\n",
    "  nBYAlltest4D     = tupleTestSpread[3]\n",
    "  # nBYtest4D        = tupleTestSpread[4]\n",
    "  nBYtest3D        = tupleTestSpread[5]\n",
    "\n",
    "  # Design the model by running the following function:\n",
    "  oModel = fBuildModel(iDepthX, iDepthX, 3)  #n_outputs, n_timesteps, n_features   #was 2 features\n",
    "\n",
    "  # Train the model by providing the X values (99,800 x 20),\n",
    "  # and the Z values (which are the cumulative Y values):\n",
    "  if iAction == 1:\n",
    "    oModelHistory = oModel.fit(nBXtrain3D, nBYtrain3D, epochs=iEpochs, verbose=1, validation_data=(nBXtest3D, nBYtest3D))\n",
    "    oModel.save(sModelFileFullPath)\n",
    "    history_dict = oModelHistory.history\n",
    "    loss_values = history_dict['loss']\n",
    "    val_loss_values = history_dict['val_loss']\n",
    "    epochs = range(1, len(loss_values) + 1)\n",
    "    plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "  oModelLoaded = keras.models.load_model(sModelFileFullPath)\n",
    "\n",
    "  if iAction == 33: #Not used now\n",
    "  # Generate the predictions from the model on the test dataset of 200 x 20 records,\n",
    "  # by iterating though each of the 200, loading the output into oPredicted, and then reshaped into nPredictedFinal\n",
    "    oPredicted = []\n",
    "    # nBXtest3Dold = nBXtest4D[2,:,:,:]\n",
    "    nBXtesting3D = nBXAlltest4D[2,:,:,3:6]\n",
    "\n",
    "    for i in range(nBXtesting3D.shape[0]):\n",
    "      print(\"Row: \" , str(i))\n",
    "      oRow2D = nBXtesting3D[i]\n",
    "      oRowReshaped3D = oRow2D.reshape(1,nBXtesting3D.shape[1],3)  #1,5,1  # was 2\n",
    "\n",
    "      oRowPredicted = oModelLoaded.predict(oRowReshaped3D)\n",
    "      oPredicted.append(oRowPredicted)\n",
    "\n",
    "    nPredicted = np.array(oPredicted)\n",
    "    nPredictedFinal = nPredicted.reshape(nPredicted.shape[0],nPredicted.shape[2])\n",
    "\n",
    "\n",
    "  # Generate the predictions from the test part of the source file, and save them into \n",
    "  # value 6 in nPredictions4D (which is a copy of nBXAlltest4D), with value 7 having the Y actual values\n",
    "  if iAction == 2:   \n",
    "    print(\"ModelFit3\")\n",
    "    nPredictions4Draw = nBXAlltest4D.copy()\n",
    "    nZeros = np.zeros(shape=(nBXAlltest4D.shape[0],nBXAlltest4D.shape[1],nBXAlltest4D.shape[2],2))\n",
    "    nPredictions4D = np.append(nPredictions4Draw, nZeros, axis=3)\n",
    "    \n",
    "    for iStockIndex in range(nBXAlltest4D.shape[0]):\n",
    "      nBXAlltestOneStock3D = nBXAlltest4D[iStockIndex,:,:,:]\n",
    "      print(\"Row: \" , str(iStockIndex))\n",
    "      for iRunIndex in range(nBXAlltestOneStock3D.shape[0]):\n",
    "        nBXAlltestOneStockOneRunRecord2D = nBXAlltestOneStock3D[iRunIndex,:,:]\n",
    "        nPredictInput2D = nBXAlltestOneStockOneRunRecord2D[:,3:6]\n",
    "        nPredictInput3D = nPredictInput2D.reshape(1,nPredictInput2D.shape[0],3)\n",
    "        nPredictTarget = nBYAlltest4D[iStockIndex,iRunIndex,:,4]\n",
    "        nPredictions4D[iStockIndex,iRunIndex,:,7] = nPredictTarget\n",
    "        oRowPredictedRaw = oModelLoaded.predict(nPredictInput3D)\n",
    "        oRowPredicted = oRowPredictedRaw.reshape(oRowPredictedRaw.shape[1])\n",
    "        nPredictions4D[iStockIndex,iRunIndex,:,6] = oRowPredicted\n",
    "        print(  \"Row: \" , str(iStockIndex)\n",
    "              , \" Run: \", str(iRunIndex)\n",
    "              , \" Prediction: \", str(nPredictions4D[iStockIndex,iRunIndex,6,6])\n",
    "              , \" Target: \", str(nPredictions4D[iStockIndex,iRunIndex,6,7])\n",
    "              )\n",
    "        #if iStockIndex == 2 and iRunIndex == 30:\n",
    "        #  oFred2 = nPredictions4D[iStockIndex,iRunIndex,:,:]\n",
    "        #  oFred = nPredictInput3D\n",
    "\n",
    "    np.save(sPredictionsFileFullPath, nPredictions4D)\n",
    "\n",
    "  nPredictions4D = np.load(sPredictionsFileFullPath)\n",
    "\n",
    "  # fPrint(nPredictions4D[2, 30, 6, :])\n",
    "  # fPrint(nPredictions4D[2, 30, 19, 3])  #0.02\n",
    "  # fPrint(nPredictions4D[2, 37, 19, 3])  #0.1\n",
    "\n",
    "  # fPrint(nPredictions4D[2,30,5,7])  #0.0004791183753658501\n",
    "  # fPrint(nPredictions4D[2,30,6,7])  #0.10052703021290244\n",
    "\n",
    "  # fPrint(nPredictions4D[2,30,5,6])  #0.0011756770545616746\n",
    "  # fPrint(nPredictions4D[2,30,6,6])  #0.09948894381523132\n",
    "\n",
    "  # fPrint(nPredictions4D[2,29,6,6])\t#0.0052693127654492855\n",
    "  # fPrint(nPredictions4D[2,29,7,6])\t#0.008663389831781387\n",
    "\n",
    "  # fPrint(nPredictions4D[2,29,6,7])\t#0.0204887007428729\n",
    "  # fPrint(nPredictions4D[2,29,7,7])\t#0.12253757081716032\n",
    "\n",
    "  # print(\"Social1 spike:\")\n",
    "\n",
    "  # fPrint(nPredictions4D[2, 39, 19, 4])\t#-0.019608\n",
    "  # fPrint(nPredictions4D[2, 49, 19, 3])  #-0.090909090909091\n",
    "  \n",
    "  # fPrint(nPredictions4D[2, 39, 8, 7])   #0.021082481702138667\n",
    "  # fPrint(nPredictions4D[2, 39, 9, 7])   #-0.07174319845260135\n",
    "\n",
    "  # fPrint(nPredictions4D[2, 39, 8, 6])   #0.010913881473243237\n",
    "  # fPrint(nPredictions4D[2, 39, 9, 6])   #-0.07884889841079712\n",
    "\n",
    "  # fPrint(nPredictions4D[2, 38, 9, 6])   #0.014400970190763474\n",
    "  # fPrint(nPredictions4D[2, 38, 10, 6])  #0.016602344810962677\n",
    "\n",
    "  # fPrint(nPredictions4D[2, 38, 9, 7])   #0.02024992907694667\n",
    "  # fPrint(nPredictions4D[2, 38, 10, 7])  #-0.07250006447550317\n",
    "\n",
    "  # print(\"Social2 spike:\")\n",
    "\n",
    "  # fPrint(nPredictions4D[2, 3, 19, 5])\t  #-0.019608\n",
    "  # fPrint(nPredictions4D[2, 18, 19, 3])\t#-0.090909090909091\n",
    "\n",
    "  # fPrint(nPredictions4D[2, 3, 13, 7])   #-0.0005855898365195733\n",
    "  # fPrint(nPredictions4D[2, 3, 14, 7])   #-0.09144144530592702\n",
    "\n",
    "  # fPrint(nPredictions4D[2, 3, 13, 6])   #0.030499937012791634\n",
    "  # fPrint(nPredictions4D[2, 3, 14, 6])   #-0.060842156410217285\n",
    "\n",
    "  # fPrint(nPredictions4D[2, 2, 14, 6])   #0.018615230917930603\n",
    "  # fPrint(nPredictions4D[2, 2, 15, 6])   #0.018593236804008484\n",
    "\n",
    "  # fPrint(nPredictions4D[2, 2, 14, 7])   #-0.0008223257331689826\n",
    "  # fPrint(nPredictions4D[2, 2, 15, 7])   #-0.09165665975742654\n",
    "\n",
    "\n",
    "  fProfit = float(0)  # set the datatype to float\n",
    "  fProfitAbs = float(0)\n",
    "  iTrades = 0\n",
    "  # Generate the trades that would occur if the trade is done where\n",
    "  # the prediction is >= the value in fPredictTarget below\n",
    "  if iAction == 3:\n",
    "    print(\"Generating Trades..\")\n",
    "  \n",
    "    iStockIndex = 0\n",
    "    iRunIndex = 0\n",
    "    iSpreadIndex = 0\n",
    "  \n",
    "    iStockIndexMax = nPredictions4D.shape[0]\n",
    "    iRunIndexMax = 0\n",
    "    iSpreadIndexMax = 0\n",
    "    oTrade = []\n",
    "    oTrades = []\n",
    "\n",
    "    while (iStockIndex <= iStockIndexMax-1):\n",
    "      print(\"Processing StockI: \")\n",
    "      print(iStockIndex)\n",
    "      nPredictionsStock3D = nPredictions4D[iStockIndex,:,:,:]\n",
    "      iRunIndex = 0\n",
    "      iRunIndexMax = nPredictionsStock3D.shape[0]\n",
    "      while (iRunIndex <= iRunIndexMax-1):\n",
    "        nPredictionsStockRun2D = nPredictionsStock3D[iRunIndex,:,:]\n",
    "        iSpreadIndex = 0\n",
    "        iSpreadIndexMax = nPredictionsStockRun2D.shape[0]\n",
    "        while (iSpreadIndex <= iSpreadIndexMax-1):\n",
    "          nPredictionsStockRunSpread1D = nPredictionsStockRun2D[iSpreadIndex,:]\n",
    "          fPrediction = nPredictionsStockRunSpread1D[6]\n",
    "          fActual = nPredictionsStockRunSpread1D[7]\n",
    "          if fPrediction >= fPredictTarget:\n",
    "            iTrades = iTrades + 1\n",
    "            fProfit = fProfit + fActual\n",
    "            fProfitAbs = fProfitAbs + abs(fActual)\n",
    "            print(\"Stock: \", str(iStockIndex), \" Run: \"\n",
    "                  , iRunIndex, \" Spread: \", iSpreadIndex\n",
    "                  , \" fPrediction: \", fPrediction, \" fActual: \", fActual)\n",
    "            oTrade = [iStockIndex, iRunIndex, iSpreadIndex, -1, -1, -1, fPrediction, fActual]\n",
    "            oTrades.append(oTrade)\n",
    "            iRunIndex = iRunIndex + iSpreadIndex #move forward to next trade potential\n",
    "            iSpreadIndex = iSpreadIndexMax # break out of loop\n",
    "          iSpreadIndex = iSpreadIndex + 1\n",
    "        iRunIndex = iRunIndex + 1\n",
    "      iStockIndex = iStockIndex + 1\n",
    "\n",
    "    nTrades2D = np.array(oTrades)\n",
    "    np.save(sTradesFileFullPath, nTrades2D)\n",
    "\n",
    "      # for iStockIndex in range(nPredictions4D.shape[0]):\n",
    "      #   nPredictionsStock3D = nPredictions4D[iStockIndex,:,:,:]\n",
    "      #   for iRunIndex in range(nPredictionsStock3D.shape[0]):\n",
    "      #     nPredictionsStockRun2D = nPredictionsStock3D[iRunIndex,:,:]\n",
    "      #     for iSpreadIndex in range(nPredictionsStockRun2D.shape[0]):\n",
    "      #       nPredictionsStockRunSpread1D = nPredictionsStockRun2D[iSpreadIndex,:]\n",
    "      #       fPrediction = nPredictionsStockRunSpread1D[6]\n",
    "      #       fActual = nPredictionsStockRunSpread1D[7]\n",
    "      #       if iSpreadIndex == 1:\n",
    "      #         iSpreadIndex = 7\n",
    "      #       if fPrediction >= 2:\n",
    "      #         iTrades = iTrades + 1\n",
    "      #         fProfit = fProfit + fActual\n",
    "      #         fProfitAbs = fProfitAbs + abs(fActual)\n",
    "      #         # print(\"gt 0.5b\")\n",
    "      #\n",
    "  # End of if iAction == 3:\n",
    "  \n",
    "\n",
    "  nPredictionsOverTarget3 = np.where(nPredictions4D[:,:,:,6] >= fPredictTarget)\n",
    "  nPredictionsOverTarget2 = np.array(nPredictionsOverTarget3)\n",
    "  nPredictionsOverTarget = np.transpose(nPredictionsOverTarget2)\n",
    "  print(\"fPredictTarget: \", fPredictTarget, \" nPredictionsOverTarget: \", nPredictionsOverTarget.shape[0])\n",
    "\n",
    "  nTrades2D = np.load(sTradesFileFullPath)\n",
    "\n",
    "  for nTrades1D in nTrades2D:\n",
    "    fPrediction = nTrades1D[6]\n",
    "    fProfit = fProfit + nTrades1D[7]\n",
    "    fProfitAbs = fProfitAbs + abs(nTrades1D[7])\n",
    "    print(iTrades, fPrediction, nTrades1D[7])\n",
    "    iTrades += 1\n",
    "\n",
    "  print(\"end\")\n",
    "  print(\"end\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NeilM.COLORADO\\.conda\\envs\\envLeopard\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\NeilM.COLORADO\\.conda\\envs\\envLeopard\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row:  0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Row:  1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Row:  2\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Row:  3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Row:  4\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Row:  5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  6\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Row:  7\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Row:  8\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Row:  9\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Row:  10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  11\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  12\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  13\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Row:  14\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  16\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Row:  17\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  18\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  19\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Row:  20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  21\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  22\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  23\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  24\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  25\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  26\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  27\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  28\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Row:  29\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Row:  30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Row:  31\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Row:  32\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Row:  33\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  34\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  35\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Row:  36\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Row:  37\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  38\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  39\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  41\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  43\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Row:  44\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Row:  45\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Row:  46\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Row:  47\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Row:  48\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Row:  49\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Row:  51\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Row:  52\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  53\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  54\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Row:  55\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Row:  56\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Row:  57\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Row:  58\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  59\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Row:  60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  61\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  62\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Row:  63\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  64\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Row:  65\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Row:  66\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Row:  67\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  68\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Row:  69\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  70\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Row:  71\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  72\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Row:  73\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Row:  74\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Row:  75\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Row:  76\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Row:  77\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  78\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Row:  79\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Row:  80\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Row:  81\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  82\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Row:  83\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  84\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  85\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Row:  86\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Row:  87\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Row:  88\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Row:  89\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "Row:  90\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Row:  91\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Row:  92\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Row:  93\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  94\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Row:  95\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  96\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  97\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Row:  98\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Row:  99\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Row:  100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  101\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Row:  102\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  103\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  104\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  105\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  106\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Row:  107\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  108\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Row:  109\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  110\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Row:  111\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Row:  112\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Row:  113\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  114\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  115\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Row:  116\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  117\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  118\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  119\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Row:  120\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Row:  121\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  122\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  123\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  124\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  125\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Row:  126\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  127\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  128\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  129\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  130\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Row:  131\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Row:  132\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Row:  133\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Row:  134\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Row:  135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Row:  136\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Row:  137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  138\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Row:  139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  140\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Row:  141\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Row:  142\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Row:  143\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  144\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Row:  145\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Row:  146\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  147\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  148\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  149\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  150\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  151\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  152\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Row:  153\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  154\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Row:  155\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Row:  156\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Row:  157\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  158\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Row:  159\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "fPredictTarget:  0  nPredictionsOverTarget:  24474\n",
      "0 45.070274353027344 -0.01679125369359291\n",
      "1 41.72657775878906 -0.056266114954772406\n",
      "2 41.6694450378418 -0.18233419554929176\n",
      "3 46.628257751464844 0.03424105573419034\n",
      "4 41.90827560424805 -0.032759023423910905\n",
      "5 41.08445358276367 0.06893987875784835\n",
      "6 41.049049377441406 0.20840011450652418\n",
      "7 40.63665008544922 0.04776154295084689\n",
      "8 40.52493667602539 0.004566067379170313\n",
      "end\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "iRunMain = 1\n",
    "\n",
    "if iRunMain == 1:\n",
    "  output = fMain(\n",
    "        iAction=33  # 0 to read the files only, 1 to train the model\n",
    "          #, 2 to run the predictions, 3 to generate the trades to be run\n",
    "          #, 33 to generate oPredictionsFinal, a numpy array of the predictions\n",
    "      , sIsAzureYN = \"N\"    # Y for Azure, N for Windows\n",
    "      , sModelFileName = \"modelLeopardF.keras\"\n",
    "      , sSourceFileName = \"LeopardF.csv\"\n",
    "      , sPredictionsFileName = \"PredictionsF.npy\"\n",
    "      , sTradesFileName = \"TradesF.npy\"\n",
    "      , fPredictTarget = 0\n",
    "      , iEpochs = 5\n",
    "      , iNumTestSplitRecordsPerStock = 200\n",
    "      , iDepthX = 20\n",
    "      , iDepthY = 20\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
